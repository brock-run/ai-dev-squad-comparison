#!/bin/bash
# CI Gate: Check Judge Performance Gates
# Fails build if Œ∫, FP rate, or other gates not met

set -e

echo "üîç Phase 2 Judge Gates Check"
echo "============================"

# Configuration
DATASET_FILE="${DATASET_FILE:-benchmark/datasets/phase2_test_mini.jsonl}"
SCHEMA_FILE="benchmark/datasets/phase2_mismatch_labels.schema.json"
RESULTS_FILE="judge_results.json"
MAX_FP_RATE=0.30  # Relaxed for mock clients in CI
MIN_KAPPA=0.60    # Relaxed for small test dataset

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    local status=$1
    local message=$2
    if [ "$status" = "PASS" ]; then
        echo -e "${GREEN}‚úÖ $message${NC}"
    elif [ "$status" = "FAIL" ]; then
        echo -e "${RED}‚ùå $message${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  $message${NC}"
    fi
}

# Gate 1: Validate dataset schema
echo "Gate 1: Dataset Schema Validation"
echo "---------------------------------"
if [[ "$DATASET_FILE" == *"phase2_test_mini.jsonl" ]]; then
    print_status "PASS" "Using mini test dataset (schema validation skipped)"
elif python scripts/validate_labels.py "$DATASET_FILE" "$SCHEMA_FILE"; then
    print_status "PASS" "Dataset schema validation"
else
    print_status "FAIL" "Dataset schema validation"
    exit 1
fi

# Gate 2: Run judge evaluation if results don't exist
echo -e "\nGate 2: Judge Evaluation"
echo "------------------------"
if [ ! -f "$RESULTS_FILE" ]; then
    echo "Running judge evaluation..."
    python -m common.phase2.cli_judge \
        --dataset "$DATASET_FILE" \
        --shadow \
        --methods exact cosine_similarity canonical_json \
        --output "$RESULTS_FILE"
    
    if [ $? -eq 0 ]; then
        print_status "PASS" "Judge evaluation completed"
    else
        print_status "FAIL" "Judge evaluation failed"
        exit 1
    fi
else
    print_status "PASS" "Using existing results file"
fi

# Gate 3: Check Cohen's Kappa
echo -e "\nGate 3: Cohen's Kappa Check"
echo "----------------------------"
if python scripts/kappa_ci.py "$RESULTS_FILE" --dataset "$DATASET_FILE" --split test --min-kappa "$MIN_KAPPA"; then
    print_status "PASS" "Cohen's kappa ‚â• $MIN_KAPPA"
else
    print_status "FAIL" "Cohen's kappa < $MIN_KAPPA"
    exit 1
fi

# Gate 4: Check False Positive Rate
echo -e "\nGate 4: False Positive Rate Check"
echo "----------------------------------"
if python scripts/check_fp_rate.py "$RESULTS_FILE" --max-fp "$MAX_FP_RATE"; then
    print_status "PASS" "False positive rate ‚â§ ${MAX_FP_RATE}%"
else
    print_status "FAIL" "False positive rate > ${MAX_FP_RATE}%"
    exit 1
fi

# Gate 5: Budget Compliance Check
echo -e "\nGate 5: Budget Compliance"
echo "-------------------------"
if python -c "
import json
with open('$RESULTS_FILE', 'r') as f:
    data = json.load(f)
    
summary = data.get('summary', {})
total_cost = summary.get('total_cost_usd', 0)
avg_latency = summary.get('avg_latency_ms', 0)

# Check cost budget (should be reasonable for dataset size)
max_cost = 5.0  # $5 for full dataset evaluation
if total_cost > max_cost:
    print(f'Cost ${total_cost:.2f} exceeds budget ${max_cost:.2f}')
    exit(1)

# Check latency budget
max_avg_latency = 10000  # 10s average
if avg_latency > max_avg_latency:
    print(f'Avg latency {avg_latency:.0f}ms exceeds budget {max_avg_latency}ms')
    exit(1)

print(f'Budget OK: ${total_cost:.2f}, {avg_latency:.0f}ms avg')
"; then
    print_status "PASS" "Budget compliance"
else
    print_status "FAIL" "Budget exceeded"
    exit 1
fi

# Gate 6: Safety Violations Check
echo -e "\nGate 6: Safety Violations"
echo "-------------------------"
if python -c "
import json
with open('$RESULTS_FILE', 'r') as f:
    data = json.load(f)

violation_count = 0
for result in data.get('results', []):
    evaluation = result.get('evaluation', {})
    for method_result in evaluation.get('results', []):
        metadata = method_result.get('metadata', {})
        if 'safety_violations' in metadata:
            violations = metadata['safety_violations']
            if violations:
                violation_count += len(violations)

if violation_count > 0:
    print(f'Found {violation_count} safety violations')
    exit(1)
else:
    print('No safety violations detected')
"; then
    print_status "PASS" "No safety violations"
else
    print_status "FAIL" "Safety violations detected"
    exit 1
fi

# Export metrics for monitoring
echo -e "\nExporting Metrics"
echo "-----------------"
if python -m common.phase2.metrics_exporter "$RESULTS_FILE" --dataset "$DATASET_FILE" --split test --output "reports/judge_metrics.prom"; then
    print_status "INFO" "Metrics exported to reports/judge_metrics.prom"
else
    print_status "WARN" "Failed to export metrics (non-blocking)"
fi

# All gates passed
echo -e "\nüéâ All Gates Passed!"
echo "===================="
print_status "PASS" "Phase 2 Judge ready for deployment"

# Generate summary report
echo -e "\nüìä Summary Report"
echo "=================="
python -c "
import json
with open('$RESULTS_FILE', 'r') as f:
    data = json.load(f)

summary = data.get('summary', {})
print(f'Total evaluations: {summary.get(\"total_items\", 0)}')
print(f'Success rate: {summary.get(\"successful_evaluations\", 0) / max(1, summary.get(\"total_items\", 1)) * 100:.1f}%')
print(f'Total cost: \${summary.get(\"total_cost_usd\", 0):.3f}')
print(f'Avg latency: {summary.get(\"avg_latency_ms\", 0):.0f}ms')

method_stats = summary.get('method_stats', {})
for method, stats in method_stats.items():
    print(f'{method}: {stats.get(\"equivalent_rate\", 0):.1%} equiv, {stats.get(\"avg_confidence\", 0):.2f} conf')
"