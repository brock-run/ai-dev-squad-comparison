# ===== Core environment =====
AI_DEV_SQUAD_ENV=development
AI_DEV_SQUAD_SEED=42

# Ollama (local LLMs)
AI_DEV_SQUAD_OLLAMA_BASE_URL=http://localhost:11434
AI_DEV_SQUAD_MODEL_DEFAULT=llama3.1:8b
AI_DEV_SQUAD_MODEL_CODER=codellama:13b
AI_DEV_SQUAD_MODEL_SMALL=llama3.2:3b

# Per-role overrides (optional)
AI_DEV_SQUAD_MODEL_ARCHITECT=
AI_DEV_SQUAD_MODEL_DEVELOPER=
AI_DEV_SQUAD_MODEL_QA=

# ===== Safety & Policy =====
AI_DEV_SQUAD_SAFETY_SANDBOX=docker   # docker | subprocess
AI_DEV_SQUAD_SAFETY_TIMEOUT_SECONDS=300
AI_DEV_SQUAD_NETWORK_DEFAULT_DENY=true
AI_DEV_SQUAD_NETWORK_ALLOWLIST=api.github.com,api.gitlab.com

# ===== VCS (tokens optional for advisory) =====
AI_DEV_SQUAD_VCS_PROVIDER=github     # github | gitlab | none
GITHUB_TOKEN=
GITLAB_TOKEN=

# ===== Telemetry =====
AI_DEV_SQUAD_LOG_LEVEL=INFO
AI_DEV_SQUAD_OTEL_EXPORTER_ENDPOINT=http://localhost:4317

# ===== Adapters toggle (comma-separated) =====
AI_DEV_SQUAD_ADAPTERS_ENABLED=langgraph,crewai,autogen,n8n,semantic_kernel,langroid,llamaindex,haystack,strands

# ===== n8n (optional) =====
N8N_HOST=http://localhost:5678