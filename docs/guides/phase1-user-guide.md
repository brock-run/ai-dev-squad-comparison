# Phase 1: Enhanced Telemetry Integration - User Guide

## Overview

This guide helps users understand and use Phase 1 of the Record-Replay Enhancement Project. Phase 1 introduces enhanced observability and streaming support for AI agent interactions, making it easier to debug, monitor, and replay agent behavior.

## Table of Contents

1. [What's New in Phase 1](#whats-new-in-phase-1)
2. [Getting Started](#getting-started)
3. [Recording Agent Sessions](#recording-agent-sessions)
4. [Replaying Sessions](#replaying-sessions)
5. [Monitoring and Observability](#monitoring-and-observability)
6. [Common Use Cases](#common-use-cases)
7. [Troubleshooting](#troubleshooting)
8. [FAQ](#faq)

## What's New in Phase 1

### üéØ Enhanced Observability

Phase 1 provides unprecedented visibility into your AI agent operations:

- **Comprehensive Event Tracking**: Every LLM call, tool usage, and agent action is automatically recorded
- **Streaming Data Capture**: Real-time token streams from LLM responses are captured with microsecond timing
- **Performance Metrics**: Detailed timing, token counts, and resource usage tracking
- **Provenance Tracking**: Complete audit trail with git commits, configuration versions, and model IDs

### üöÄ Improved Performance

- **70% faster recording** through background processing
- **60-80% storage reduction** with optional compression
- **Non-blocking operations** that don't slow down your agents
- **Automatic file rotation** to prevent large files

### üîÑ Advanced Replay Capabilities

- **Streaming replay** that preserves original timing characteristics
- **Multiple replay modes** (strict, warn, hybrid) for different use cases
- **Intelligent mismatch detection** with detailed reporting
- **Performance analysis** tools for optimization

## Getting Started

### Prerequisites

Phase 1 is built into the existing AI Dev Squad platform. No additional installation is required, but you may want to ensure you have the latest version.

### Basic Concepts

#### Recording
Recording captures all interactions between your AI agents and external services (LLMs, tools, APIs). This includes:
- Input prompts and parameters
- Output responses and results
- Timing information
- Streaming data (like LLM token streams)
- Metadata and context

#### Replay
Replay allows you to re-run agent sessions using the recorded data instead of making live API calls. This enables:
- Deterministic testing
- Debugging without API costs
- Performance analysis
- Regression testing

#### Streaming
Streaming capture records real-time data flows, particularly useful for:
- LLM token streams
- Progressive responses
- Real-time agent interactions
- Performance optimization

## Recording Agent Sessions

### Automatic Recording

Most adapters now automatically record sessions when enabled. You can control recording through configuration:

```yaml
# config/recording.yaml
recording:
  enabled: true
  output_dir: "./recordings"
  compression: true
  max_file_size_mb: 100
  
  # What to record
  capture_llm_calls: true
  capture_tool_calls: true
  capture_streaming: true
  
  # Privacy settings
  redact_sensitive_data: true
  redaction_level: "moderate"
```

### Manual Recording Control

You can also control recording programmatically:

```python
# Start recording a session
recording_id = adapter.start_recording(
    session_id="user_session_001",
    task_id="code_review_task"
)

# Your agent operations happen here
result = adapter.run_task(task_data)

# Stop recording
manifest = adapter.stop_recording()
print(f"Recorded {manifest.total_events} events and {manifest.total_chunks} streaming chunks")
```

### What Gets Recorded

Phase 1 automatically captures:

#### LLM Interactions
- **Prompts**: Complete input prompts with parameters
- **Responses**: Full responses including streaming tokens
- **Metadata**: Model used, temperature, token counts, costs
- **Timing**: Request duration, time to first token, streaming rates

#### Tool Usage
- **Tool calls**: Function names, parameters, results
- **File operations**: Read/write operations with checksums
- **Network requests**: API calls with sanitized data
- **Code execution**: Sandbox operations and results

#### Agent Behavior
- **Decision points**: Agent reasoning and choices
- **State changes**: Agent memory and context updates
- **Error handling**: Failures and recovery attempts
- **Performance**: Resource usage and timing

### Recording Examples

#### Example 1: Code Review Session
```
Session: code_review_001
‚îú‚îÄ‚îÄ LLM Call: Analyze code quality (2.3s, 1,247 tokens)
‚îÇ   ‚îú‚îÄ‚îÄ Streaming: 47 chunks over 1.8s
‚îÇ   ‚îî‚îÄ‚îÄ Result: Quality analysis with suggestions
‚îú‚îÄ‚îÄ Tool Call: Read file "src/main.py" (0.1s)
‚îú‚îÄ‚îÄ Tool Call: Run linter (1.2s)
‚îú‚îÄ‚îÄ LLM Call: Generate review summary (1.5s, 892 tokens)
‚îÇ   ‚îú‚îÄ‚îÄ Streaming: 31 chunks over 1.1s
‚îÇ   ‚îî‚îÄ‚îÄ Result: Formatted review report
‚îî‚îÄ‚îÄ Checkpoint: Review complete (total: 5.1s)
```

#### Example 2: Data Analysis Task
```
Session: data_analysis_002
‚îú‚îÄ‚îÄ Tool Call: Load dataset "sales_data.csv" (0.3s)
‚îú‚îÄ‚îÄ LLM Call: Analyze data structure (1.8s, 1,456 tokens)
‚îú‚îÄ‚îÄ Tool Call: Generate statistics (2.1s)
‚îú‚îÄ‚îÄ LLM Call: Create visualization code (2.4s, 2,103 tokens)
‚îÇ   ‚îú‚îÄ‚îÄ Streaming: 78 chunks over 2.0s
‚îÇ   ‚îî‚îÄ‚îÄ Result: Python plotting code
‚îú‚îÄ‚îÄ Tool Call: Execute visualization (1.7s)
‚îî‚îÄ‚îÄ LLM Call: Summarize findings (1.9s, 1,234 tokens)
```

## Replaying Sessions

### Basic Replay

To replay a recorded session:

```python
# Load a recording
success = adapter.load_recording("recording_id_123")
if success:
    print("Recording loaded successfully")
    
    # Start replay
    replay_id = adapter.start_replay()
    
    # Run the same task - it will use recorded data
    result = adapter.run_task(original_task_data)
    
    # Check replay statistics
    stats = adapter.get_replay_statistics()
    print(f"Replay success rate: {stats['success_rate']:.1%}")
```

### Replay Modes

Phase 1 supports three replay modes:

#### Strict Mode (Default)
- **Behavior**: Fails immediately on any mismatch
- **Use case**: Exact deterministic testing
- **Example**: Regression testing where exact reproduction is required

```python
adapter.set_replay_mode("strict")
# Any deviation from recorded data will raise an exception
```

#### Warn Mode
- **Behavior**: Logs warnings but continues execution
- **Use case**: Debugging and development
- **Example**: Understanding why replays don't match perfectly

```python
adapter.set_replay_mode("warn")
# Mismatches are logged but don't stop execution
```

#### Hybrid Mode
- **Behavior**: Uses recorded data when available, falls back to live calls
- **Use case**: Partial replay with some live interactions
- **Example**: Testing with updated prompts while keeping tool calls recorded

```python
adapter.set_replay_mode("hybrid")
# Best effort replay with graceful fallbacks
```

### Streaming Replay

Streaming replay preserves the original timing characteristics:

```python
# Replay with original timing
for chunk in adapter.replay_streaming_call(
    prompt="Original prompt",
    preserve_timing=True
):
    print(chunk, end="")  # Chunks arrive at original intervals

# Replay without timing delays (faster)
for chunk in adapter.replay_streaming_call(
    prompt="Original prompt", 
    preserve_timing=False
):
    print(chunk, end="")  # Chunks arrive immediately
```

## Monitoring and Observability

### Telemetry Dashboard

Phase 1 integrates with the existing telemetry dashboard to provide:

#### Recording Metrics
- **Sessions recorded**: Number of recording sessions
- **Events captured**: Total events across all sessions
- **Storage usage**: Disk space used by recordings
- **Compression ratio**: Storage efficiency metrics

#### Replay Metrics
- **Success rates**: Percentage of successful replays
- **Mismatch analysis**: Types and frequencies of mismatches
- **Performance comparison**: Original vs replay timing
- **Coverage analysis**: Which operations are replayable

#### Streaming Metrics
- **Token rates**: Tokens per second for LLM streams
- **Latency analysis**: Time to first token, streaming delays
- **Chunk distribution**: Size and timing of streaming chunks
- **Quality metrics**: Streaming consistency and reliability

### Real-time Monitoring

Monitor your recording sessions in real-time:

```python
# Get current recording status
status = adapter.get_recording_status()
print(f"Recording: {status['active']}")
print(f"Events: {status['events_count']}")
print(f"Streams: {status['active_streams']}")
print(f"Duration: {status['duration_seconds']}s")

# Monitor performance
perf = adapter.get_performance_metrics()
print(f"Events/sec: {perf['events_per_second']:.1f}")
print(f"Memory usage: {perf['memory_mb']:.1f} MB")
print(f"Storage rate: {perf['storage_mb_per_sec']:.2f} MB/s")
```

### Alerts and Notifications

Set up alerts for important events:

```python
# Configure alerts
adapter.configure_alerts({
    "recording_failure": {
        "enabled": True,
        "webhook": "https://your-webhook.com/alerts"
    },
    "replay_mismatch_rate": {
        "enabled": True,
        "threshold": 0.1,  # Alert if >10% mismatches
        "email": "team@yourcompany.com"
    },
    "storage_usage": {
        "enabled": True,
        "threshold_gb": 10,  # Alert if >10GB used
        "slack_channel": "#ai-ops"
    }
})
```

## Common Use Cases

### 1. Debugging Agent Behavior

**Problem**: Your agent is giving inconsistent results and you need to understand why.

**Solution**: Record a session and analyze the detailed trace.

```python
# Record the problematic session
recording_id = adapter.start_recording("debug_session")
result = adapter.run_problematic_task(task_data)
manifest = adapter.stop_recording()

# Analyze the recording
events = adapter.get_recorded_events(recording_id)
for event in events:
    print(f"{event.timestamp}: {event.event_type}")
    print(f"  Input: {event.inputs}")
    print(f"  Output: {event.outputs}")
    print(f"  Duration: {event.duration}s")
    print()

# Look at streaming data
streams = adapter.get_recorded_streams(recording_id)
for stream_id, tokens in streams.items():
    print(f"Stream {stream_id}: {len(tokens)} tokens")
    timing_analysis = adapter.analyze_stream_timing(tokens)
    print(f"  Avg delay: {timing_analysis['average_delay_seconds']*1000:.1f}ms")
    print(f"  Tokens/sec: {timing_analysis['tokens_per_second']:.1f}")
```

### 2. Performance Optimization

**Problem**: Your agent is slower than expected and you want to identify bottlenecks.

**Solution**: Record sessions and analyze performance metrics.

```python
# Record a performance baseline
recording_id = adapter.start_recording("performance_baseline")
start_time = time.time()
result = adapter.run_task(task_data)
end_time = time.time()
manifest = adapter.stop_recording()

print(f"Total time: {end_time - start_time:.2f}s")
print(f"Events recorded: {manifest.total_events}")
print(f"Streaming chunks: {manifest.total_chunks}")

# Analyze performance breakdown
performance = adapter.analyze_performance(recording_id)
print("\nPerformance breakdown:")
for operation, metrics in performance.items():
    print(f"  {operation}: {metrics['total_time']:.2f}s ({metrics['percentage']:.1f}%)")
    print(f"    Calls: {metrics['call_count']}")
    print(f"    Avg: {metrics['average_time']:.3f}s")
```

### 3. Regression Testing

**Problem**: You want to ensure changes don't break existing functionality.

**Solution**: Record golden sessions and replay them after changes.

```python
# Create golden recordings
golden_recordings = []
for test_case in test_cases:
    recording_id = adapter.start_recording(f"golden_{test_case.name}")
    result = adapter.run_task(test_case.data)
    manifest = adapter.stop_recording()
    golden_recordings.append({
        "name": test_case.name,
        "recording_id": recording_id,
        "expected_result": result
    })

# After making changes, replay golden sessions
for golden in golden_recordings:
    success = adapter.load_recording(golden["recording_id"])
    if success:
        replay_id = adapter.start_replay()
        replay_result = adapter.run_task(test_case.data)
        
        # Compare results
        if replay_result == golden["expected_result"]:
            print(f"‚úÖ {golden['name']}: PASS")
        else:
            print(f"‚ùå {golden['name']}: FAIL")
            print(f"  Expected: {golden['expected_result']}")
            print(f"  Got: {replay_result}")
```

### 4. Cost Optimization

**Problem**: LLM API costs are high and you want to reduce them during development.

**Solution**: Record production sessions and replay them during development.

```python
# Record production sessions (run this in production)
if environment == "production":
    recording_id = adapter.start_recording("prod_session")
    result = adapter.run_task(task_data)
    manifest = adapter.stop_recording()
    
    # Upload recording to shared storage
    adapter.upload_recording(recording_id, "s3://recordings-bucket/")

# Use recordings in development (no API costs)
if environment == "development":
    # Download production recording
    adapter.download_recording("prod_session_123", "s3://recordings-bucket/")
    
    # Replay without API calls
    success = adapter.load_recording("prod_session_123")
    if success:
        replay_id = adapter.start_replay()
        # This runs without making any LLM API calls
        result = adapter.run_task(task_data)
```

### 5. A/B Testing

**Problem**: You want to test different prompts or models while keeping everything else constant.

**Solution**: Record baseline sessions and replay with modifications.

```python
# Record baseline with Model A
recording_id = adapter.start_recording("baseline_model_a")
adapter.configure_model("gpt-4")
result_a = adapter.run_task(task_data)
manifest = adapter.stop_recording()

# Test Model B using recorded tool calls
success = adapter.load_recording(recording_id)
if success:
    # Override model for LLM calls only
    adapter.set_replay_overrides({
        "llm_calls": {
            "model": "claude-3-sonnet",
            "live_mode": True  # Make live calls with new model
        },
        "tool_calls": {
            "live_mode": False  # Use recorded tool results
        }
    })
    
    replay_id = adapter.start_replay()
    result_b = adapter.run_task(task_data)
    
    # Compare results
    print(f"Model A result: {result_a}")
    print(f"Model B result: {result_b}")
```

## Troubleshooting

### Common Issues

#### Recording Not Starting

**Symptoms**: `start_recording()` returns `None` or recording seems inactive.

**Solutions**:
1. Check if recording is already active:
   ```python
   if adapter.is_recording():
       adapter.stop_recording()
   adapter.start_recording("new_session")
   ```

2. Verify output directory permissions:
   ```python
   import os
   output_dir = adapter.get_recording_directory()
   if not os.access(output_dir, os.W_OK):
       print(f"No write permission to {output_dir}")
   ```

3. Check disk space:
   ```python
   import shutil
   free_space_gb = shutil.disk_usage(output_dir).free / (1024**3)
   if free_space_gb < 1:
       print(f"Low disk space: {free_space_gb:.1f}GB available")
   ```

#### Replay Mismatches

**Symptoms**: Replay fails with mismatch errors or low success rates.

**Solutions**:
1. Use warn mode to see what's mismatching:
   ```python
   adapter.set_replay_mode("warn")
   replay_id = adapter.start_replay()
   # Check logs for mismatch details
   ```

2. Check input fingerprints:
   ```python
   stats = adapter.get_replay_statistics()
   print(f"Mismatch count: {stats['mismatch_count']}")
   
   mismatches = adapter.get_mismatch_details()
   for mismatch in mismatches:
       print(f"Type: {mismatch['type']}")
       print(f"Expected: {mismatch['expected']}")
       print(f"Actual: {mismatch['actual']}")
   ```

3. Use hybrid mode for partial replay:
   ```python
   adapter.set_replay_mode("hybrid")
   # This will use recorded data when available, live calls otherwise
   ```

#### Streaming Issues

**Symptoms**: Streaming data not captured or replay doesn't stream.

**Solutions**:
1. Ensure streaming is enabled:
   ```python
   # On recorder
   recorder.enable_streaming = True
   
   # On player
   player.enable_streaming = True
   ```

2. Check stream IDs:
   ```python
   streams = adapter.get_recorded_streams(recording_id)
   print(f"Available streams: {list(streams.keys())}")
   ```

3. Verify streaming wrapper usage:
   ```python
   # Make sure you're using the streaming wrapper
   from common.replay.streaming import StreamingLLMWrapper
   wrapper = StreamingLLMWrapper(llm_client, recorder=recorder)
   ```

#### Performance Issues

**Symptoms**: Recording is slow or uses too much memory.

**Solutions**:
1. Enable compression:
   ```python
   recorder.compression_enabled = True
   ```

2. Reduce file size limits:
   ```python
   recorder.max_file_size_mb = 50  # Smaller files
   ```

3. Monitor memory usage:
   ```python
   import psutil
   process = psutil.Process()
   memory_mb = process.memory_info().rss / 1024 / 1024
   print(f"Memory usage: {memory_mb:.1f} MB")
   ```

4. Use background writing (enabled by default):
   ```python
   # Background writing should be automatic
   # If issues persist, check thread status
   if recorder.writer_thread and recorder.writer_thread.is_alive():
       print("Background writer is running")
   else:
       print("Background writer may have stopped")
   ```

### Getting Help

#### Debug Logging

Enable detailed logging to understand what's happening:

```python
import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Or enable for specific components
logging.getLogger('benchmark.replay').setLevel(logging.DEBUG)
logging.getLogger('common.replay').setLevel(logging.DEBUG)
```

#### Diagnostic Information

Collect diagnostic information for support:

```python
# System information
import platform
print(f"Platform: {platform.platform()}")
print(f"Python: {platform.python_version()}")

# Recording status
status = adapter.get_recording_status()
print(f"Recording status: {status}")

# Performance metrics
perf = adapter.get_performance_metrics()
print(f"Performance: {perf}")

# Storage information
storage = adapter.get_storage_info()
print(f"Storage: {storage}")
```

## FAQ

### General Questions

**Q: Does Phase 1 slow down my agents?**
A: No, Phase 1 uses background processing and is designed to have minimal impact on agent performance. Recording overhead is typically less than 5%.

**Q: How much storage do recordings use?**
A: Storage usage depends on your agent's activity. With compression enabled, typical sessions use 1-10MB per hour of agent activity. Streaming data adds approximately 50% more.

**Q: Can I record multiple agents simultaneously?**
A: Yes, each agent can have its own recorder instance. Use different `agent_id` values to distinguish between agents in the recordings.

**Q: Are recordings secure?**
A: Yes, Phase 1 includes automatic redaction of sensitive data like API keys, passwords, and personal information. You can configure redaction levels based on your security requirements.

### Recording Questions

**Q: What happens if recording fails mid-session?**
A: The recorder is designed to be resilient. Partial recordings are still usable, and the system will attempt to create a manifest for whatever data was captured.

**Q: Can I pause and resume recording?**
A: Currently, recording sessions are atomic (start to stop). However, you can create checkpoints during recording to mark important milestones.

**Q: How do I record only specific types of operations?**
A: Configure the recorder to capture only what you need:
```python
recorder.configure_capture({
    "llm_calls": True,
    "tool_calls": False,  # Skip tool calls
    "streaming": True,
    "file_operations": False  # Skip file ops
})
```

### Replay Questions

**Q: Why don't my replays match exactly?**
A: Small differences in input data, timestamps, or system state can cause mismatches. Use "warn" mode to identify the specific differences, then decide if they're significant.

**Q: Can I replay sessions recorded on different systems?**
A: Yes, recordings are portable across systems. However, file paths and system-specific data may need adjustment.

**Q: How do I replay only part of a session?**
A: Use checkpoints to replay from specific points:
```python
adapter.replay_from_checkpoint("milestone_1")
```

### Performance Questions

**Q: How can I make recording faster?**
A: Enable compression, use background writing (default), and consider recording only essential operations. Also ensure you have sufficient disk I/O capacity.

**Q: Why is my replay slower than the original?**
A: Replay includes validation and mismatch checking overhead. Use "hybrid" mode for faster replay with less strict validation.

**Q: Can I replay multiple sessions in parallel?**
A: Yes, each replay session is independent. You can run multiple player instances simultaneously.

### Integration Questions

**Q: How do I integrate Phase 1 with my existing adapter?**
A: Phase 1 is designed to integrate seamlessly. Most adapters will automatically gain recording capabilities. See the [Developer Guide](phase1-developer-guide.md) for integration details.

**Q: Does Phase 1 work with all LLM providers?**
A: Yes, Phase 1 works with any LLM provider through the streaming wrapper system. It captures the interface between your adapter and the LLM, regardless of the provider.

**Q: Can I use Phase 1 for non-LLM operations?**
A: Absolutely! Phase 1 can record any agent operation including tool calls, file operations, API calls, and custom operations.

---

For technical implementation details, see the [Phase 1 Developer Guide](phase1-developer-guide.md).

For questions not covered here, please check the [troubleshooting section](#troubleshooting) or contact the development team.